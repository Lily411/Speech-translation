# -*- coding: utf-8 -*-
"""speech-translation-ko.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OaksuJn6ITc8jg_XDsWaLsGWs-g7g7OI
"""

!pip install -U bitsandbytes
!pip install -U transformers
!pip install -U peft
!pip install pydub
!pip install ffmpeg-python
!pip install sounddevice

from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor, AutoModelForSpeechSeq2Seq, AutomaticSpeechRecognitionPipeline, AutoModelForCausalLM
from peft import PeftConfig, PeftModel
import torch

# MT
device = 'cpu'
if torch.cuda.is_available():
  device = "cuda"

model_id = 'meta-llama/Meta-Llama-3-8B'
token = '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'

mt_tokenizer = AutoTokenizer.from_pretrained(model_id, token = token,trust_remote_code=True)
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    token=token,
    torch_dtype=torch.float16,
    device_map = device
)
lora_model = '/content/mt'

model = PeftModel.from_pretrained(base_model, lora_model)
model.to(device)
#-------------------------------------------------------------------------------------------------------
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""

# ASR
language = "Korean"
language_decode = 'ko'
task = "transcribe"
model_name_or_path = "openai/whisper-large-v2"
model_dir = r'/content/asr'

# 从预训练模型加载特征提取器
feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
# 从预训练模型加载分词器，可以指定语言和任务以获得最适合特定需求的分词器配置
asr_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
# 从预训练模型加载处理器，处理器通常结合了特征提取器和分词器，为特定任务提供一站式的数据预处理
processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)

peft_config = PeftConfig.from_pretrained(model_dir)
base_model = AutoModelForSpeechSeq2Seq.from_pretrained(peft_config.base_model_name_or_path, device_map="auto")
peft_model = PeftModel.from_pretrained(base_model, model_dir)

peft_pipeline = AutomaticSpeechRecognitionPipeline(model = peft_model, tokenizer = asr_tokenizer, feature_extractor = feature_extractor)
forced_decoder_ids = processor.get_decoder_prompt_ids(language=language_decode, task=task)

from IPython.display import HTML, Audio
from google.colab.output import eval_js
from base64 import b64decode
import numpy as np
from scipy.io.wavfile import read as wav_read
import io
import ffmpeg

AUDIO_HTML = """
<script>
var base64data = 0;
var reader;
var recorder, gumStream;

var handleSuccess = function(stream) {
  gumStream = stream;

  var options = {
    mimeType: 'audio/webm;codecs=opus',
    audio: {
      sampleRate: 16000
    }
  };

  try {
    recorder = new MediaRecorder(stream, options);
  } catch (e) {
    recorder = new MediaRecorder(stream);
  }

  recorder.ondataavailable = function(e) {
    reader = new FileReader();
    reader.readAsDataURL(e.data);
    reader.onloadend = function() {
      base64data = reader.result;
    }
  };

  recorder.start();

  setTimeout(function() {
    if (recorder && recorder.state == "recording") {
      recorder.stop();
      gumStream.getAudioTracks()[0].stop();
    }
  }, 5000);
};

navigator.mediaDevices.getUserMedia({
  audio: {
    sampleRate: 16000
  }
}).then(handleSuccess).catch(function(err) {
  console.error(err);
});

var data = new Promise(resolve => {
  var checkInterval = setInterval(function() {
    if (base64data !== 0) {
      clearInterval(checkInterval);
      resolve(base64data.toString());
    }
  }, 500);
});
</script>
"""

def get_audio():
    display(HTML(AUDIO_HTML), display_id='audio_recorder')
    data = eval_js("data")
    binary = b64decode(data.split(',')[1])

    process = (ffmpeg
        .input('pipe:0')
        .output('pipe:1',
               format='wav',
               ar=16000,
               ac=1)
        .run_async(pipe_stdin=True, pipe_stdout=True, pipe_stderr=True, quiet=True, overwrite_output=True)
    )
    output, err = process.communicate(input=binary)

    riff_chunk_size = len(output) - 8
    q = riff_chunk_size
    b = []
    for i in range(4):
        q, r = divmod(q, 256)
        b.append(r)

    riff = output[:4] + bytes(b) + output[8:]
    sr, audio = wav_read(io.BytesIO(riff))

    return audio, sr

import warnings
warnings.filterwarnings("ignore")

while True:
  with torch.cuda.amp.autocast():
    audio_data, sample_rate = get_audio()

    speech_text = peft_pipeline(audio_data, max_new_tokens=255)["text"]
    print(speech_text)

    eval_prompt = 'translate korean to chinese:\n input:' + speech_text + '\noutput:'
    inputs = mt_tokenizer(
            [
                alpaca_prompt.format(
                    "translate korean to chinese:",  # instruction
                    speech_text,# input
                    "",  # output - leave this blank for generation!
                )
            ], return_tensors="pt").to("cuda")

    outputs = model.generate(**inputs, max_new_tokens=255, use_cache=True)
    de_output = mt_tokenizer.batch_decode(outputs)
    mt_text = de_output[0][de_output[0].index("Response:") + len("Response:\n"):de_output[0].index("<|end_of_text|>") ]

    print(mt_text)